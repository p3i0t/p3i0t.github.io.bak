<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-12-23T16:28:35+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Wang Xin’s Site</title><subtitle>This is my personal site with some of my notes and posts about my research.</subtitle><entry><title type="html">Summary of “Adversarial Examples Are a Natural Consequence of Test Error in Noise”</title><link href="http://localhost:4000/2019/12/22/Adversarial&Corruption-Robustness.html" rel="alternate" type="text/html" title="Summary of &quot;Adversarial Examples Are a Natural Consequence of Test Error in Noise&quot;" /><published>2019-12-22T00:00:00+08:00</published><updated>2019-12-22T00:00:00+08:00</updated><id>http://localhost:4000/2019/12/22/Adversarial&amp;Corruption-Robustness</id><content type="html" xml:base="http://localhost:4000/2019/12/22/Adversarial&amp;Corruption-Robustness.html">&lt;p&gt;Adversarial examples pose a serious threat to machine learning models. However, they are not the only “illegal inputs” that pose a threat. Another type of inputs images with natural and common corruptions, e.g. additive Gaussian noises.&lt;/p&gt;

&lt;p&gt;So far, the researchers seem to work seperately, and keep them two lines of research. This paper, titled
 &lt;a href=&quot;https://arxiv.org/abs/1901.10513&quot;&gt;Adversarial Examples Are a Natural Consequence of Test Error in Noise[1]&lt;/a&gt;
  establishes the connection between these two kinds of robustness: adversarial robustness and corruption robustness.&lt;/p&gt;

&lt;h2 id=&quot;adversarial-and-corruption-robustness&quot;&gt;Adversarial and Corruption Robustness&lt;/h2&gt;

&lt;p&gt;Both of them are defined as functions of &lt;em&gt;error set&lt;/em&gt; of a statistical classifier. We  denote $E$ the error set, which is the set of points in the input space on which the classifier makes incorrect predictions.&lt;/p&gt;

&lt;h3 id=&quot;adversarial-robustness&quot;&gt;Adversarial Robustness&lt;/h3&gt;

&lt;p&gt;Let p represents the clean image distribution, and $d(x, E)$ denote the distance from $x$ to the nearest point in $E$. Then adversarial robustness $\mathcal{P}_{x\sim p}[d(x, E) &amp;lt; \epsilon]$, the probability that some random sample from $p$ is not within distance $\epsilon$ of any point in the error set $E$.&lt;/p&gt;

&lt;h3 id=&quot;corruption-robustness&quot;&gt;Corruption Robustness&lt;/h3&gt;

&lt;p&gt;Let $q$ represent the corrupted image distribution. Corruption robustness is $\mathcal{P}_{x\sim q}[x \notin E ]$.&lt;/p&gt;

&lt;h2 id=&quot;adversarial-robustness-and-gaussian-noise-corruption-robustness-are-highly-correlated&quot;&gt;Adversarial Robustness and Gaussian Noise Corruption Robustness are highly correlated&lt;/h2&gt;

&lt;p&gt;One big take-away is while adversarial training performs better, Gaussian data augmentation does improve adversarial robustness. And in the oppsite direction, adversarial training  helps model against noise corruptions. One thing to note is Gaussian noise is just one of the many corruptions in ImageNet-C dataset (15 in total).&lt;/p&gt;

&lt;p&gt;The authors suggest that researchers should take adversarial robustness and corruption robustness into considertaion at the same time. Reasons are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Corruptions may expose failure modes of a model that we might miss. Adversatial training improves adversarial robustness, but degrades performance on the fog and contrast corruptions.&lt;/li&gt;
  &lt;li&gt;Measuring corruption robustness is significantly easier than measuring adversarial robustness. Computing adversarial robustness perfectly requires solving an NP-hard problem for all points in the test set. That’s possibly why hundreds of adversarial defense papers published are successfully fooled later. Since correctly evaluate and report adversarial robustness is hard.&lt;/li&gt;
  &lt;li&gt;Failed adversarial defense strategies also fail to improve robustness against Gausssian noise. So the claimed $L_p$ adversarial robustness improvement doesn’t implies robustness of distribution shift due to various corruptions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[1] Adversarial examples are a natural consequence of test error in noise, Ford, Nic and Gilmer, Justin and Carlini, Nicolas and Cubuk, Dogus, ICML 2019&lt;/p&gt;</content><author><name></name></author><summary type="html">Adversarial examples pose a serious threat to machine learning models. However, they are not the only “illegal inputs” that pose a threat. Another type of inputs images with natural and common corruptions, e.g. additive Gaussian noises.</summary></entry><entry><title type="html">Problem of Frechet Inception Distance(FID) score and Alternative</title><link href="http://localhost:4000/2019/10/25/FID-problems.html" rel="alternate" type="text/html" title="Problem of Frechet Inception Distance(FID) score and Alternative" /><published>2019-10-25T00:00:00+08:00</published><updated>2019-10-25T00:00:00+08:00</updated><id>http://localhost:4000/2019/10/25/FID-problems</id><content type="html" xml:base="http://localhost:4000/2019/10/25/FID-problems.html">&lt;p&gt;Frechet Inception Distance (FID) score is measure for the quality of GAN’s generated samples. Given a set of generated samples and a set of real images, we first extract the 2048-dimensional activations from the pool3 layer of &lt;a href=&quot;https://tfhub.dev/google/imagenet/inception_v3/classification/4&quot;&gt;Inception-v3&lt;/a&gt; model; Then use a multi-variate Gaussian distribution to model both sets of activations. Denote $\mu_g, \Sigma_g$ and $\mu_r, \Sigma_r$ are the meas and covariances of generated images and real images. FID is calculated as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;FID = ||\mu_x - \mu_g||_2^2 + \text{Tr}(\Sigma_x + \Sigma_g - 2(\Sigma_x\Sigma_g)^\frac{1}{2})&lt;/script&gt;

&lt;p&gt;Besides relying on the pretrained Inception-v3 model, the biggest problem of FID is using single-modal multi-variate Gaussian to model the activations, which could be complex and multi-modal.&lt;/p&gt;

&lt;p&gt;It is very easy to construct an example of FID’s failure. Consider the following three $2d$ distributions:&lt;/p&gt;

&lt;p&gt;Distribution A, with 4 modes (each a $2d$ Gaussian):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;mu_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# means of 4 modes
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# all have the same covariance.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Distribution B, with 4 modes (each a $2d$ Gaussian):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sqrt_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mu_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# means of 4 modes
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# all have the same covariance.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Distribution C, with only 1 mode (also $2d$ Gaussian):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/fid_dists.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we try to capture these three &lt;em&gt;different&lt;/em&gt; distributions with one single $2d$ Gaussian, they all have the same $\mu, \Sigma$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which implies that $FID(A, B) = FID(A, C) = FID(B, C) = 0$.&lt;/p&gt;</content><author><name></name></author><summary type="html">Frechet Inception Distance (FID) score is measure for the quality of GAN’s generated samples. Given a set of generated samples and a set of real images, we first extract the 2048-dimensional activations from the pool3 layer of Inception-v3 model; Then use a multi-variate Gaussian distribution to model both sets of activations. Denote $\mu_g, \Sigma_g$ and $\mu_r, \Sigma_r$ are the meas and covariances of generated images and real images. FID is calculated as:</summary></entry><entry><title type="html">Deep Infomax: Unsupervised Representation Learning by Maximizing Mutual Information</title><link href="http://localhost:4000/2019/10/21/Test-post.html" rel="alternate" type="text/html" title="Deep Infomax: Unsupervised Representation Learning by Maximizing Mutual Information " /><published>2019-10-21T00:00:00+08:00</published><updated>2019-10-21T00:00:00+08:00</updated><id>http://localhost:4000/2019/10/21/Test-post</id><content type="html" xml:base="http://localhost:4000/2019/10/21/Test-post.html">&lt;p&gt;Deep Infomax is an unsupervised representation learning framework which maximize
the mutual information(MI) between the inputs and outputs of an encoder.&lt;/p&gt;</content><author><name></name></author><summary type="html">Deep Infomax is an unsupervised representation learning framework which maximize the mutual information(MI) between the inputs and outputs of an encoder.</summary></entry></feed>